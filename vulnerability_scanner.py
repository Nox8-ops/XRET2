import requests
from bs4 import BeautifulSoup
import json
import time
import argparse
from datetime import datetime
from termcolor import colored
from selenium import webdriver

# شعار الأداة
def print_logo():
    logo = """
    ██████╗ ██╗  ██╗███████╗████████╗
    ██╔══██╗██║  ██║██╔════╝╚══██╔══╝
    ██████╔╝███████║█████╗     ██║   
    ██╔═══╝ ██╔══██║██╔══╝     ██║   
    ██║     ██║  ██║███████╗   ██║   
    ╚═╝     ╚═╝  ╚═╝╚══════╝   ╚═╝   

    XRET Vulnerability Scanner v2.4.0-beta
    Developed by @Nox8
    """
    print(colored(logo, 'cyan'))

# تحسينات في التقارير
def save_report(vulnerabilities, report_file="report.json"):
    with open(report_file, 'w') as file:
        json.dump(vulnerabilities, file, indent=4)
    print(colored(f"Report saved to {report_file}", 'yellow'))

def add_screenshot(url, screenshot_file="screenshot.png"):
    try:
        options = webdriver.ChromeOptions()
        options.add_argument('--headless')
        driver = webdriver.Chrome(options=options)
        driver.get(url)
        driver.save_screenshot(screenshot_file)
        driver.quit()
        print(colored(f"Screenshot saved to {screenshot_file}", 'yellow'))
    except Exception as e:
        print(colored(f"Error capturing screenshot: {e}", 'red'))

# تعزيز الأمان الشبكي
def ssl_tls_check(url):
    try:
        response = requests.get(url, verify=True)
        print(colored(f"SSL/TLS check passed for {url}", 'green'))
    except requests.exceptions.SSLError as e:
        print(colored(f"SSL/TLS check failed for {url}: {e}", 'red'))

def external_links_check(url):
    try:
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')
        external_links = [a['href'] for a in soup.find_all('a', href=True) if url not in a['href']]
        for link in external_links:
            try:
                requests.get(link)
                print(colored(f"External link {link} is accessible", 'green'))
            except requests.exceptions.RequestException as e:
                print(colored(f"External link {link} is not accessible: {e}", 'red'))
    except requests.exceptions.RequestException as e:
        print(colored(f"Failed to check external links for {url}: {e}", 'red'))

# تحسين طرق تجاوز 403/401
def bypass_403_401(url):
    headers_list = [
        {'X-Forwarded-For': '127.0.0.1'},
        {'X-Original-URL': '/admin'},
        {'X-Override-URL': '/admin'},
        {'X-Rewrite-URL': '/admin'},
    ]
    for headers in headers_list:
        try:
            response = requests.get(url, headers=headers)
            if response.status_code != 403 and response.status_code != 401:
                print(colored(f"Successfully bypassed 403/401 with headers {headers}", 'green'))
                return
        except requests.exceptions.RequestException as e:
            print(colored(f"Error bypassing 403/401 with headers {headers}: {e}", 'red'))

    print(colored("Failed to bypass 403/401", 'red'))

# إضافة وكلاء أو بروكسيات
def proxy_check(url, proxies):
    for proxy in proxies:
        try:
            response = requests.get(url, proxies={'http': proxy, 'https': proxy})
            print(colored(f"Accessed {url} with proxy {proxy}", 'green'))
        except requests.exceptions.RequestException as e:
            print(colored(f"Error accessing {url} with proxy {proxy}: {e}", 'red'))

# استخدام مجموعة متنوعة من User-Agent headers
def user_agent_bypass(url, user_agents):
    for user_agent in user_agents:
        headers = {'User-Agent': user_agent}
        try:
            response = requests.get(url, headers=headers)
            print(colored(f"Accessed {url} with User-Agent {user_agent}", 'green'))
        except requests.exceptions.RequestException as e:
            print(colored(f"Error accessing {url} with User-Agent {user_agent}: {e}", 'red'))

# دمج المكونات والفحص الكامل
def scan(url, proxies=None, user_agents=None):
    vulnerabilities = []

    print_logo()

    # تحسينات في التقارير
    add_screenshot(url)

    # تعزيز الأمان الشبكي
    ssl_tls_check(url)
    external_links_check(url)

    # تحسين طرق تجاوز 403/401
    bypass_403_401(url)

    # إضافة وكلاء أو بروكسيات
    if proxies:
        proxy_check(url, proxies)

    # استخدام مجموعة متنوعة من User-Agent headers
    if user_agents:
        user_agent_bypass(url, user_agents)

    # حفظ التقرير
    save_report(vulnerabilities)

def main():
    parser = argparse.ArgumentParser(description='XRET Vulnerability Scanner v2.4.0-beta')
    parser.add_argument('url', nargs='?', help='The target URL to scan')
    parser.add_argument('--proxies', nargs='*', help='List of proxies to use')
    parser.add_argument('--user-agents', nargs='*', help='List of User-Agents to use')
    parser.add_argument('--list-user-agents', action='store_true', help='List available User-Agents')

    args = parser.parse_args()

    if args.list_user_agents:
        print("Available User-Agents:")
        user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/12.1.2 Safari/605.1.15",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:68.0) Gecko/20100101 Firefox/68.0"
        ]
        for ua in user_agents:
            print(f"- {ua}")
        return

    if not args.url:
        parser.print_help()
        return

    scan(args.url, proxies=args.proxies, user_agents=args.user_agents)

if __name__ == "__main__":
    main()
