import requests
from bs4 import BeautifulSoup
import json
import argparse
import logging
from datetime import datetime
from termcolor import colored
from selenium import webdriver
from selenium.webdriver.chrome.service import Service as ChromeService
from webdriver_manager.chrome import ChromeDriverManager
from tqdm import tqdm

# إعداد سجل الأحداث
logging.basicConfig(level=logging.INFO, filename='xret_scanner.log', filemode='a', 
                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

def print_logo():
    logo = """
    ██████╗ ██╗  ██╗███████╗████████╗
    ██╔══██╗██║  ██║██╔════╝╚══██╔══╝
    ██████╔╝███████║█████╗     ██║   
    ██╔═══╝ ██╔══██║██╔══╝     ██║   
    ██║     ██║  ██║███████╗   ██║   
    ╚═╝     ╚═╝  ╚══════╝   ╚═╝   

    XRET Vulnerability Scanner v2.4.0-beta
    Developed by @Nox8
    """
    print(colored(logo, 'cyan'))

def save_report(vulnerabilities, report_file="report.json"):
    with open(report_file, 'w') as file:
        json.dump(vulnerabilities, file, indent=4)
    print(colored(f"Report saved to {report_file}", 'yellow'))
    logging.info(f"Report saved to {report_file}")

def add_screenshot(url, screenshot_file="screenshot.png"):
    try:
        options = webdriver.ChromeOptions()
        options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        service = ChromeService(executable_path=ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)
        driver.get(url)
        driver.save_screenshot(screenshot_file)
        driver.quit()
        print(colored(f"Screenshot saved to {screenshot_file}", 'yellow'))
        logging.info(f"Screenshot saved to {screenshot_file}")
    except Exception as e:
        print(colored(f"Error capturing screenshot: {e}", 'red'))
        logging.error(f"Error capturing screenshot: {e}")

def ssl_tls_check(url):
    try:
        response = requests.get(url, verify=True)
        print(colored(f"SSL/TLS check passed for {url}", 'green'))
        logging.info(f"SSL/TLS check passed for {url}")
    except requests.exceptions.SSLError as e:
        print(colored(f"SSL/TLS check failed for {url}: {e}", 'red'))
        logging.error(f"SSL/TLS check failed for {url}: {e}")

def external_links_check(url):
    try:
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')
        external_links = [a['href'] for a in soup.find_all('a', href=True) if url not in a['href']]
        for link in tqdm(external_links, desc="Checking external links"):
            try:
                requests.get(link)
                print(colored(f"External link {link} is accessible", 'green'))
                logging.info(f"External link {link} is accessible")
            except requests.exceptions.RequestException as e:
                print(colored(f"External link {link} is not accessible: {e}", 'red'))
                logging.error(f"External link {link} is not accessible: {e}")
    except requests.exceptions.RequestException as e:
        print(colored(f"Failed to check external links for {url}: {e}", 'red'))
        logging.error(f"Failed to check external links for {url}: {e}")

def bypass_403_401(url):
    headers_list = [
        {'X-Forwarded-For': '127.0.0.1'},
        {'X-Original-URL': '/admin'},
        {'X-Override-URL': '/admin'},
        {'X-Rewrite-URL': '/admin'},
    ]
    for headers in tqdm(headers_list, desc="Bypassing 403/401"):
        try:
            response = requests.get(url, headers=headers)
            if response.status_code not in [403, 401]:
                print(colored(f"Successfully bypassed 403/401 with headers {headers}", 'green'))
                logging.info(f"Successfully bypassed 403/401 with headers {headers}")
                return
        except requests.exceptions.RequestException as e:
            print(colored(f"Error bypassing 403/401 with headers {headers}: {e}", 'red'))
            logging.error(f"Error bypassing 403/401 with headers {headers}: {e}")
    print(colored("Failed to bypass 403/401", 'red'))
    logging.info("Failed to bypass 403/401")

def proxy_check(url, proxies):
    for proxy in tqdm(proxies, desc="Checking proxies"):
        try:
            response = requests.get(url, proxies={'http': proxy, 'https': proxy})
            print(colored(f"Accessed {url} with proxy {proxy}", 'green'))
            logging.info(f"Accessed {url} with proxy {proxy}")
        except requests.exceptions.RequestException as e:
            print(colored(f"Error accessing {url} with proxy {proxy}: {e}", 'red'))
            logging.error(f"Error accessing {url} with proxy {proxy}: {e}")

def user_agent_bypass(url, user_agents):
    for user_agent in tqdm(user_agents, desc="Testing User-Agents"):
        headers = {'User-Agent': user_agent}
        try:
            response = requests.get(url, headers=headers)
            print(colored(f"Accessed {url} with User-Agent {user_agent}", 'green'))
            logging.info(f"Accessed {url} with User-Agent {user_agent}")
        except requests.exceptions.RequestException as e:
            print(colored(f"Error accessing {url} with User-Agent {user_agent}: {e}", 'red'))
            logging.error(f"Error accessing {url} with User-Agent {user_agent}: {e}")

def scan(url, proxies=None, user_agents=None):
    vulnerabilities = []
    print_logo()
    add_screenshot(url)
    ssl_tls_check(url)
    external_links_check(url)
    bypass_403_401(url)
    if proxies:
        proxy_check(url, proxies)
    if user_agents:
        user_agent_bypass(url, user_agents)
    save_report(vulnerabilities)

def show_usage():
    usage = """
    Usage: python xret_scanner.py [options] <url>
    
    Options:
      --proxies              List of proxies to use
      --user-agents          List of User-Agents to use
      --list-user-agents     List available User-Agents
      --no-scan              Show usage information without performing a scan
      --help                 Show this message and exit
    
    Example:
      python xret_scanner.py https://example.com --proxies http://proxy1 http://proxy2 --user-agents "Mozilla/5.0" "Chrome/70.0"
    """
    print(usage)

def main():
    parser = argparse.ArgumentParser(description='XRET Vulnerability Scanner v2.4.0-beta', add_help=False)
    parser.add_argument('url', nargs='?', help='The target URL to scan')
    parser.add_argument('--proxies', nargs='*', help='List of proxies to use')
    parser.add_argument('--user-agents', nargs='*', help='List of User-Agents to use')
    parser.add_argument('--list-user-agents', action='store_true', help='List available User-Agents')
    parser.add_argument('--no-scan', action='store_true', help='Show usage information without performing a scan')
    parser.add_argument('--help', action='store_true', help='Show usage information')

    args = parser.parse_args()

    if args.help or args.no_scan:
        show_usage()
        return

    if args.list_user_agents:
        print("Available User-Agents:")
        user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/12.1.2 Safari/605.1.15",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:68.0) Gecko/20100101 Firefox/68.0"
        ]
        for ua in user_agents:
            print(f"- {ua}")
        return

    if not args.url:
        parser.print_help()
        return

    scan(args.url, proxies=args.proxies, user_agents=args.user_agents)

if __name__ == "__main__":
    main()
